{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arabic CHATBOT - PROJASK Academy (Trial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an Example on Retrieval chatbots of PROJASK academy with very simple dataset as trial using two methods TF-IDF and LSTM \n",
    "\n",
    "\n",
    "\n",
    "#### Our Toolbox\n",
    "We’re going to use Python 3 along some libraries which include:\n",
    "\n",
    "* Keras\n",
    "* Scikit-learn\n",
    "* Pandas\n",
    "\n",
    "## Types of Chat Bots\n",
    "\n",
    "* Retrieval Based\n",
    "* Generative\n",
    "\n",
    "Each approach has its use case, based on your application and your abilities you decide which one to use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Retrieval bots \n",
    "It works by having a pre defined data set of Questions/Answers and a similarity measure to decide which question in the data set is most similar to the one asked.\n",
    "\n",
    "* No grammatical mistakes\n",
    "* No Irrelavent Answers\n",
    "* Less Data to work\n",
    "\n",
    "Steps:\n",
    "* Encode questions in vectors using pre-defined method\n",
    "* Using Predefined similarity method, find the most similar question in DB\n",
    "* Return the answer of the choosen questions \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generative\n",
    "Models work by training a neural network using NLP techniques to output an answer given a certain input question without needing a data set to lookup.\n",
    "\n",
    "\n",
    "#### Open Domain & Closed Domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set \n",
    "Arabic NLP is a competitive field with new breakthroughs everyday. The complexity of Arabic makes it hard to perform cognitive tasks without being lost in the details.\n",
    "\n",
    "It is well known that the more data we have the better we can draw analytics from and the better models we can build. I’ve recently found many very active accounts in Ask.fm that provide knowledge to people for free, this website works by submitting a question to an author and having it displayed once they reply. I gathered a list of some of these accounts and gathered as many as possible of their questions/answers pairs. They’re all Islamic questions so with a single domain I guess the data is consistent and can be useful.\n",
    "\n",
    "This dataset can be used as a base dataset for a more advanced question answering data set or it can just be used as a knowledge base where search algorithms are applied to extract useful information or even as the training data for a Chat bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"full_dataset2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Question', 'Answer'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>مرحبا بك</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>السلام عليكم</td>\n",
       "      <td>و عليكم السلام و رحمه الله و بركاته</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>اهلا و سهلا</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>كيفك</td>\n",
       "      <td>الحمدلله</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ازيك</td>\n",
       "      <td>كويس الحمدلله</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Question                               Answer\n",
       "0         مرحبا                              مرحبا بك\n",
       "1  السلام عليكم   و عليكم السلام و رحمه الله و بركاته\n",
       "2         مرحبا                           اهلا و سهلا\n",
       "3           كيفك                             الحمدلله\n",
       "4           ازيك                        كويس الحمدلله"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>مرحبا</td>\n",
       "      <td>8 شارع سابا باشا الاسكندريه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Question                       Answer\n",
       "count        31                           31\n",
       "unique       29                           17\n",
       "top      مرحبا   8 شارع سابا باشا الاسكندريه\n",
       "freq          3                            6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval based bots\n",
    "These bots rely on the similarity between the input question and all the questions in the data set. In order to compute this similarity we need to choose a similarity measure that would rate the similarity of two sentences, there are a lot of similarity measures for text but we will choose the cosine similarity for this one since it’s one of the most common measures in NLP.\n",
    "### Cosine similarity\n",
    "It works by measuring the cosine of the angle between two vectors, thus it is concerned by their directions rather than their magnitudes, which in text represents the term frequency in each question regardless of the document length, in other words the length of the documents(questions) will not affect the computation but only. You can see that in the equation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing questions\n",
    "To go from a text question to a vector that represents the question so we can compute the similarity we need to transform it, in order to transform a text document into a vector [Process of encoding text as integers to create feature vectors [numbers represent the data] ] we need to use a feature extraction technique, we will use TF-IDF because it’s the most common in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF is computed by first computing two values for each term:\n",
    "\n",
    "Term Frequency: The frequency of the term in the document\n",
    "Document Frequency: The fraction of the documents that contain the term\n",
    "Inverse Document Frequency: The logarithmically scaled inverse Document Frequency\n",
    "The term frequency is used because we are concerned with finding documents that have similar terms, because if two documents have the same terms then they are probably very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "vectorizer.fit(data.values.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our vectorizer is ready to transform any question into a vector using TFIDF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a question: \n",
      "هل لديكم كورسات؟\n",
      "لدينا كرسات برمجه python, Java, C, C++\n",
      "\n",
      " ########## \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read a question from the user\n",
    "question = [input('Please enter a question: \\n')]\n",
    "question = vectorizer.transform(question)    #Convert a collection of raw documents to a matrix of TF-IDF features.\n",
    "\n",
    "# Rank all the questions using cosine similarity to the input question\n",
    "rank = cosine_similarity(question, vectorizer.transform(data['Question'].values))\n",
    "# Grab the top 5\n",
    "top = np.argsort(rank, axis=-1).T[-1:].tolist()\n",
    "\n",
    "# Print top 5\n",
    "for item in top:\n",
    "    print(data['Answer'].iloc[item].values[0])\n",
    "    print(\"\\n ########## \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very simple question and therefore the results are good given the very simple approach we’re following."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, RepeatVector\n",
    "from keras.utils import np_utils\n",
    "from nltk.stem import ISRIStemmer\n",
    "from six.moves import cPickle\n",
    "\n",
    "BATCH_SIZE = 32 # Batch size for GPU\n",
    "NUM_WORDS = 10000 # Vocab length\n",
    "MAX_LEN = 20 # Padding length (# of words)\n",
    "LSTM_EMBED = 8 # Number of LSTM nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches_generator(train_data, batch_size=32):\n",
    "    # For OHE inputs\n",
    "    num_words = np.max(train_data) + 1\n",
    "    timesteps = train_data.shape[1]\n",
    "    while True:\n",
    "        indices = np.random.choice(len(train_data), size=batch_size)\n",
    "        X = train_data[indices]\n",
    "        X = np_utils.to_categorical(X, num_words)\n",
    "        X = X.reshape((batch_size, timesteps, num_words))\n",
    "        yield (X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"full_dataset2.csv\")\n",
    "stemmer = ISRIStemmer()         #it is the process to return the word to its roots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't need the answers, so let's drop them\n",
    "train_data.drop('Answer', inplace=True, axis=1)\n",
    "\n",
    "train_data = train_data[train_data.Question.apply(lambda x: len(x.split())) < MAX_LEN]  #Apply the max length\n",
    "\n",
    "train_data.Question = train_data.Question.apply(lambda x: (re.sub('[^\\u0620-\\uFEF0\\s]', '', x)).strip()) #remove not arabic word\n",
    "\n",
    "train_data = train_data[train_data.Question.apply(len) > 0]  #remove empty questions if founded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stem the words\n",
    "train_data.Question = train_data.Question.apply(lambda x: \" \".join([stemmer.stem(i) for i in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=NUM_WORDS, lower=False)\n",
    "\n",
    "tokenizer.fit_on_texts(train_data[\"Question\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer for later use\n",
    "cPickle.dump(tokenizer, open(\"lstm-autoencoder-tokenizer.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tokenizer.texts_to_sequences(train_data[\"Question\"].values)\n",
    "\n",
    "train_data = pad_sequences(train_data, padding='post', truncating='post', maxlen=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From c:\\users\\abdullah\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/3\n",
      "31/31 [==============================] - 3s 106ms/step - loss: 9.2101\n",
      "Epoch 2/3\n",
      "31/31 [==============================] - 1s 17ms/step - loss: 9.2078\n",
      "Epoch 3/3\n",
      "31/31 [==============================] - 1s 16ms/step - loss: 9.2053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12635b1e848>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(NUM_WORDS, 100, input_length=MAX_LEN))\n",
    "model.add(LSTM(LSTM_EMBED, dropout=0.2, recurrent_dropout=0.2, input_shape=(train_data.shape[1], NUM_WORDS)))\n",
    "model.add(RepeatVector(train_data.shape[-1]))\n",
    "model.add(LSTM(LSTM_EMBED, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model.add(Dense(NUM_WORDS, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "# model.fit_generator(batches_generator(train_data), steps_per_epoch=(len(train_data) // BATCH_SIZE))\n",
    "model.fit(train_data, np.expand_dims(train_data, -1), epochs=3, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"lstm-encoder.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from six.moves import cPickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from nltk.stem import ISRIStemmer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "BATCH_SIZE = 32 # Batch size for GPU\n",
    "NUM_WORDS = 10000 # Vocab length\n",
    "MAX_LEN = 20 # Padding length (# of words)\n",
    "LSTM_EMBED = 8 # Number of LSTM nodes\n",
    "\n",
    "K.set_learning_phase(False)\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"full_dataset2.csv\")\n",
    "tokenizer = cPickle.load(open(\"lstm-autoencoder-tokenizer.pickle\", \"rb\"))\n",
    "\n",
    "stemmer = ISRIStemmer()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the encoder model\n",
    "model = load_model(\"lstm-encoder.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoding function\n",
    "encode = K.function([model.input, K.learning_phase()], [model.layers[1].output])\n",
    "\n",
    "\n",
    "Questions = tokenizer.texts_to_sequences(data.Question)\n",
    "# We pad sequences that are shorter than MAX_LEN\n",
    "Questions = pad_sequences(Questions, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "Questions = np.squeeze(np.array(encode([Questions])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter a question: \n",
      "مواعيد العمل الرسميه؟\n",
      "لا يا فندم \n",
      "إن شاء الله\n",
      "مساء النور\n",
      "8 شارع سابا باشا الاسكندريه\n",
      "من التاسعه صباحا  و حتي العاشره مساءا\n"
     ]
    }
   ],
   "source": [
    "question = input('Please enter a question: \\n')\n",
    "question = stemmer.stem(question)\n",
    "question = tokenizer.texts_to_sequences([question])\n",
    "question = pad_sequences(question, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "question = np.squeeze(encode([question]))\n",
    "\n",
    "rank = cosine_similarity(question.reshape(1, -1), Questions)\n",
    "top = np.argsort(rank, axis=-1).T[-5:].tolist()\n",
    "for item in top:\n",
    "    print(data['Answer'].iloc[item].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input('Please enter a question: \\n')\n",
    "question = stemmer.stem(question)\n",
    "question = tokenizer.texts_to_sequences([question])\n",
    "question = pad_sequences(question, padding='post', truncating='post', maxlen=MAX_LEN)\n",
    "question = np.squeeze(encode([question]))\n",
    "\n",
    "rank = cosine_similarity(question.reshape(1, -1), Questions)\n",
    "top = np.argsort(rank, axis=-1).T[0].tolist()\n",
    "for item in top:\n",
    "    print(data['Answer'].iloc[item].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
